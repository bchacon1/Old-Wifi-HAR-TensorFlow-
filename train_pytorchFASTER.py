import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import KFold
import numpy as np
import matplotlib.pyplot as plt
import os
import sklearn.metrics as sk_metrics
from concurrent.futures import ThreadPoolExecutor
import sys # For clean exit
import datetime # For recording start time and possibly other timing
from torch.cuda.amp import autocast, GradScaler # For Mixed Precision Training (AMP)

# --- Import your model and dataset classes and parameters from pytorch_model.py ---
# Make sure pytorch_model.py contains WifiHARLSTM, WifiActivityDataset,
# and the global parameters like n_input, n_hidden, n_classes, window_size, threshold
from pytorch_model import (
    WifiHARLSTM,
    WifiActivityDataset,
    n_input,
    n_hidden,
    n_classes,
    window_size,
    raw_window_size,
    threshold,
)

# `raw_window_size` corresponds to the sequence length used when generating
# the CSV files via `cross_vali_data_convert_merge.py`.  The sequences are
# downsampled to `window_size` timesteps before being fed to the model.

# --- CONFIGURABLE PARAMETERS ---
learning_rate = 0.0001
training_epochs = 2000
batch_size = 200
display_step = 100 # How often to print training progress
validation_frequency = 5 # How often to run a full validation pass and check early stopping
early_stopping_patience = 20 # Number of validation checks with no improvement before stopping

# --- Data Loading Options ---
# Set to True to load from pre-saved .npy files (much faster after first run)
# Set to False to re-read all CSVs (slow, only needed if CSVs changed or .npy not created)
LOAD_FROM_NPY = True

# Set to True to save the combined data as .npy files after loading from CSVs
# (Only applicable if LOAD_FROM_NPY is False and CSVs are being read)
SAVE_TO_NPY_AFTER_CSV_LOAD = True

# --- Subset for Quick Testing ---
# Set to a float (e.g., 0.1 for 10%) to use only a fraction of the data for quick runs.
# Set to None or 1.0 to use the full dataset.
DATA_SUBSET_FRACTION = None # Or 0.1 for 10%

# --- DataLoader Optimizations ---
# Number of subprocesses to use for data loading. 0 means data loading happens in the main process.
# Recommended to set to the number of CPU cores or slightly less.
# On Windows, this *must* be inside `if __name__ == '__main__':`
NUM_DATALOADER_WORKERS = 8 # Adjust based on your CPU cores
PIN_MEMORY = torch.cuda.is_available() # Pin tensors to GPU memory for faster transfer
# Keep workers alive between epochs to avoid re-initializing. Set to True with num_workers > 0.
PERSISTENT_WORKERS = True # Can lead to memory leaks if not careful with dataset, but usually fine.


# --- Checkpointing Options ---
RESUME_FROM_CHECKPOINT = True # Attempt to resume if a checkpoint exists
CHECKPOINT_SAVE_FREQUENCY = 50 # Save a checkpoint every N epochs

# --- Mixed Precision Training (AMP) ---
# Automatically enables if CUDA is available, otherwise uses float32
USE_AMP = True if torch.cuda.is_available() else False

# --- Output Folder ---
OUTPUT_FOLDER_PATTERN = "PyTorch_LR{0}_BATCHSIZE{1}_NHIDDEN{2}/"
output_folder = OUTPUT_FOLDER_PATTERN.format(learning_rate, batch_size, n_hidden)
# Use exist_ok=True to avoid error if folder already exists
os.makedirs(output_folder, exist_ok=True)


# --- Device Configuration ---
# Automatically select CUDA (NVIDIA GPU), MPS (Apple Silicon GPU), or CPU
device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

# --- CUDNN Benchmark (ONLY for CUDA) ---
# This allows cuDNN to find the best algorithms for your specific hardware and input sizes,
# often leading to significant speedups.
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    print("Enabled cudnn benchmark for faster training on CUDA device.")


# =====================================================================
# === START OF MAIN EXECUTION BLOCK - THIS IS CRUCIAL FOR MULTIPROCESSING ON WINDOWS ===
# =====================================================================
if __name__ == '__main__':
    # --- RECORD START TIME ---
    start_overall_time = datetime.datetime.now()
    print(f"Training started at: {start_overall_time.strftime('%Y-%m-%d %H:%M:%S')}")

    # --- STEP 1: Load ALL processed data files (generated by cross_vali_data_convert_merge.py) ---
    # This phase can be very slow if loading from CSVs, but fast if loading from .npy
    all_features_npy_path = os.path.join(output_folder, "all_features_full.npy")
    all_labels_npy_path = os.path.join(output_folder, "all_labels_full.npy")

    all_features_full = None
    all_labels_full = None

    if LOAD_FROM_NPY and os.path.exists(all_features_npy_path) and os.path.exists(all_labels_npy_path):
        print(f"Loading data from pre-saved .npy files: {all_features_npy_path}, {all_labels_npy_path}")
        all_features_full = np.load(all_features_npy_path)
        all_labels_full = np.load(all_labels_npy_path)

        # Older .npy files may store sequences with `raw_window_size` time steps
        # and an extra "NoActivity" column.  Mirror the preprocessing applied
        # when loading from CSVs.
        if all_features_full.shape[1] == raw_window_size:
            all_features_full = all_features_full[:, ::2, :]
        if all_labels_full.shape[1] == 8:
            # Older arrays may include the "NoActivity" column and rows.
            no_activity_mask = all_labels_full[:, 0] != 2
            all_features_full = all_features_full[no_activity_mask]
            all_labels_full = all_labels_full[no_activity_mask, 1:]
        else:
            # If the column was already dropped, filter out zero rows which
            # indicate leftover NoActivity samples.
            no_activity_mask = np.sum(all_labels_full, axis=1) > 0
            if np.count_nonzero(~no_activity_mask) > 0:
                all_features_full = all_features_full[no_activity_mask]
                all_labels_full = all_labels_full[no_activity_mask]

        print("Data loaded from .npy files successfully.")
    else:
        print("Loading data from CSVs (this may take a long time)...")
        activity_labels = ["bed", "fall", "pickup", "run", "sitdown", "standup", "walk"]
        input_files_dir = "./input_files/"

        all_features_list_csv = []
        all_labels_list_csv = []


        def load_csv_pair(label):
            xx_file_path = os.path.join(
                input_files_dir,
                f"xx_{raw_window_size}_{threshold}_{label}.csv",
            )
            yy_file_path = os.path.join(
                input_files_dir,
                f"yy_{raw_window_size}_{threshold}_{label}.csv",
            )

            if not os.path.exists(xx_file_path) or not os.path.exists(yy_file_path):
                raise FileNotFoundError(
                    f"Missing data files for activity '{label}'. "
                    f"Please ensure 'cross_vali_data_convert_merge.py' ran successfully and generated "
                    f"{xx_file_path} and {yy_file_path}."
                )

            print(f"  Reading CSVs for activity: {label}...")
            features_csv = np.loadtxt(xx_file_path, delimiter=",", dtype=np.float32)
            labels_csv = np.loadtxt(yy_file_path, delimiter=",", dtype=np.float32)

            # Discard windows labeled as "NoActivity" (where the first column is
            # 2) to keep only the seven activity classes.
            no_activity_mask = labels_csv[:, 0] != 2
            labels_csv = labels_csv[no_activity_mask, 1:]
            features_csv = features_csv[no_activity_mask]

            features_csv = features_csv.reshape(-1, raw_window_size, n_input)
            features_csv = features_csv[:, ::2, :]

            return features_csv, labels_csv

        with ThreadPoolExecutor(max_workers=min(len(activity_labels), os.cpu_count() or 1)) as executor:
            results = list(executor.map(load_csv_pair, activity_labels))

        for features_csv, labels_csv in results:
            all_features_list_csv.append(features_csv)
            all_labels_list_csv.append(labels_csv)

        all_features_full = np.concatenate(all_features_list_csv, axis=0)
        all_labels_full = np.concatenate(all_labels_list_csv, axis=0)
        print("Data loaded from CSVs and concatenated successfully.")

        if SAVE_TO_NPY_AFTER_CSV_LOAD:
            print(f"Saving combined data to .npy files for faster loading next time: {output_folder}")
            np.save(all_features_npy_path, all_features_full)
            np.save(all_labels_npy_path, all_labels_full)
            print(".npy files saved.")

    # --- Apply Data Subset if specified ---
    if DATA_SUBSET_FRACTION is not None and DATA_SUBSET_FRACTION < 1.0:
        print(f"Using a subset of the data: {DATA_SUBSET_FRACTION * 100:.1f}%")
        num_samples_to_use = int(len(all_features_full) * DATA_SUBSET_FRACTION)

        # Randomly select indices for the subset
        np.random.seed(42) # For reproducibility of subset
        subset_indices = np.random.choice(len(all_features_full), num_samples_to_use, replace=False)

        all_features_full = all_features_full[subset_indices]
        all_labels_full = all_labels_full[subset_indices]

    print(f"Total Data for Training: Features shape {all_features_full.shape}, Labels shape {all_labels_full.shape}")

    # --- STEP 2: K-Fold Cross-Validation Loop ---
    num_folds = 5
    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)

    cv_accuracies = []
    confusion_matrix_sum = np.zeros((n_classes, n_classes), dtype=int)

    # Initialize GradScaler for AMP (if using)
    if USE_AMP:
        scaler = GradScaler()

    print("Starting K-Fold Cross-Validation...")

    try: # Wrap the main training loop in a try-except for graceful interruption
        for fold_idx, (train_index, val_index) in enumerate(kf.split(all_features_full, all_labels_full)):
            print(f"\n--- Starting Fold {fold_idx + 1}/{num_folds} ---")

            # Split data into training and validation sets for the current fold
            train_features, val_features = all_features_full[train_index], all_features_full[val_index]
            train_labels, val_labels = all_labels_full[train_index], all_labels_full[val_index]

            # Create PyTorch Dataset objects from the NumPy arrays
            train_dataset = WifiActivityDataset(features=train_features, labels=train_labels)
            val_dataset = WifiActivityDataset(features=val_features, labels=val_labels)

            # --- DataLoader with Speedup Optimizations ---
            train_loader = DataLoader(
                train_dataset,
                batch_size=batch_size,
                shuffle=True,
                drop_last=True,
                num_workers=NUM_DATALOADER_WORKERS, # Use multiple processes for data loading
                pin_memory=PIN_MEMORY,             # Pin host tensors to GPU memory
                persistent_workers=PERSISTENT_WORKERS # Keep workers alive between epochs
            )
            val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                drop_last=True,
                num_workers=NUM_DATALOADER_WORKERS, # Use multiple processes for data loading
                pin_memory=PIN_MEMORY,             # Pin host tensors to GPU memory
                persistent_workers=PERSISTENT_WORKERS # Keep workers alive between epochs
            )

            # --- Model, Loss, Optimizer ---
            model = WifiHARLSTM(n_input, n_hidden, n_classes).to(device)
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(model.parameters(), lr=learning_rate)

            # --- Training and Validation Metrics for this fold ---
            # These will accumulate history across potential resume points
            train_losses_history_fold = []
            val_losses_history_fold = []
            train_accuracies_history_fold = []
            val_accuracies_history_fold = []

            # --- Early Stopping Variables ---
            best_val_acc = 0.0 # Will be updated if a better validation accuracy is found
            no_improve_counter = 0 # Counts epochs without improvement

            # --- Resume from Checkpoint ---
            start_epoch = 0
            if RESUME_FROM_CHECKPOINT:
                # Logic to find the latest checkpoint for the current fold
                latest_checkpoint_file = None
                max_epoch_found = -1
                for fname in os.listdir(output_folder):
                    if fname.startswith(f"checkpoint_fold{fold_idx + 1}_epoch") and fname.endswith(".pth"):
                        try:
                            epoch_num = int(fname.split('_epoch')[1].split('.pth')[0])
                            if epoch_num > max_epoch_found:
                                max_epoch_found = epoch_num
                                latest_checkpoint_file = os.path.join(output_folder, fname)
                        except ValueError:
                            continue # Skip files not matching expected pattern

                if latest_checkpoint_file and os.path.exists(latest_checkpoint_file):
                    print(f"    Resuming training for Fold {fold_idx + 1} from checkpoint: {latest_checkpoint_file}")
                    checkpoint = torch.load(latest_checkpoint_file, map_location=device) # Map to correct device
                    model.load_state_dict(checkpoint['model_state_dict'])
                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    start_epoch = checkpoint['epoch']

                    # Load historical data for plotting continuity
                    train_losses_history_fold = checkpoint.get('train_loss_history', [])
                    val_losses_history_fold = checkpoint.get('val_loss_history', [])
                    train_accuracies_history_fold = checkpoint.get('train_acc_history', [])
                    val_accuracies_history_fold = checkpoint.get('val_acc_history', [])

                    # Update best_val_acc for early stopping if resuming
                    if val_accuracies_history_fold:
                        best_val_acc = max(val_accuracies_history_fold)
                    print(f"    Loaded model from epoch {start_epoch}. Resuming training...")
                else:
                    print(f"    No checkpoint found for Fold {fold_idx + 1}. Starting training from epoch 0.")

            # Check if fold is already completed (only applicable if resuming)
            if start_epoch >= training_epochs:
                print(f"    Fold {fold_idx + 1} already completed all {training_epochs} epochs. Skipping training loop for this fold.")
                # If skipping, ensure current fold's accuracy is added for overall CV average
                if val_accuracies_history_fold:
                    cv_accuracies.append(val_accuracies_history_fold[-1]) # Use the last recorded validation accuracy
                # Set these to empty so confusion matrix isn't calculated again
                all_val_preds_fold = []
                all_val_true_fold = []
            else: # Only run training loop if not already completed
                for epoch in range(start_epoch, training_epochs):
                    # --- Training Phase ---
                    model.train()
                    running_loss = 0.0
                    correct_train = 0
                    total_train = 0

                    for i, (inputs, labels_onehot) in enumerate(train_loader):
                        inputs, labels_onehot = inputs.to(device), labels_onehot.to(device)
                        labels_int = torch.argmax(labels_onehot, dim=1).long() # Convert one-hot to integer labels

                        optimizer.zero_grad()

                        # --- Mixed Precision (AMP) for forward and loss ---
                        with autocast("cuda", enabled=USE_AMP): #CHANGE IF NOT CU
                            outputs = model(inputs)
                            loss = criterion(outputs, labels_int)

                        # --- Mixed Precision (AMP) for backward and optimizer step ---
                        if USE_AMP:
                            scaler.scale(loss).backward() # Scale loss for AMP
                            scaler.step(optimizer)        # Update model parameters
                            scaler.update()               # Update the scaler for next iteration
                        else:
                            loss.backward()
                            optimizer.step()

                        running_loss += loss.item() * inputs.size(0)

                        _, predicted = torch.max(outputs.data, 1)
                        total_train += labels_int.size(0)
                        correct_train += (predicted == labels_int).sum().item()

                    epoch_train_loss = running_loss / len(train_loader.dataset) if len(train_loader.dataset) > 0 else 0.0
                    epoch_train_accuracy = correct_train / total_train if total_train > 0 else 0.0
                    train_losses_history_fold.append(epoch_train_loss)
                    train_accuracies_history_fold.append(epoch_train_accuracy)


                    # --- Validation Phase ---
                    # Only perform validation if (epoch + 1) is a multiple of VALIDATION_FREQUENCY OR it's the last epoch
                    if (epoch + 1) % validation_frequency == 0 or epoch == training_epochs - 1:
                        model.eval()
                        val_running_loss = 0.0
                        correct_val = 0
                        total_val = 0
                        all_val_preds_fold = []
                        all_val_true_fold = []

                        with torch.no_grad():
                            for inputs, labels_onehot in val_loader:
                                inputs, labels_onehot = inputs.to(device), labels_onehot.to(device)
                                labels_int = torch.argmax(labels_onehot, dim=1).long() # Convert one-hot to integer labels

                                # --- Mixed Precision (AMP) for validation forward pass ---
                                with autocast(enabled=USE_AMP):
                                    outputs = model(inputs)
                                    loss = criterion(outputs, labels_int)

                                val_running_loss += loss.item() * inputs.size(0)

                                _, predicted = torch.max(outputs.data, 1)
                                total_val += labels_int.size(0)
                                correct_val += (predicted == labels_int).sum().item()

                                all_val_preds_fold.extend(predicted.cpu().numpy())
                                all_val_true_fold.extend(labels_int.cpu().numpy())

                        epoch_val_loss = val_running_loss / len(val_loader.dataset) if len(val_loader.dataset) > 0 else 0.0
                        epoch_val_accuracy = correct_val / total_val if total_val > 0 else 0.0
                        val_losses_history_fold.append(epoch_val_loss)
                        val_accuracies_history_fold.append(epoch_val_accuracy)

                        # --- Display Progress ---
                        print(f"Epoch [{epoch + 1}/{training_epochs}], "
                              f"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, "
                              f"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}")

                        # --- Early stopping check ---
                        if epoch_val_accuracy > best_val_acc:
                            best_val_acc = epoch_val_accuracy
                            no_improve_counter = 0
                        else:
                            no_improve_counter += 1
                            if no_improve_counter >= early_stopping_patience:
                                print(f"Early stopping triggered at epoch {epoch+1} for fold {fold_idx+1}. Best Val Acc: {best_val_acc:.4f}")
                                # Append current validation accuracy before breaking
                                cv_accuracies.append(epoch_val_accuracy)
                                break # Break from epoch loop

                    # --- Checkpoint Saving ---
                    # Save checkpoint at specified frequency OR at the last epoch (even if early stopping broke it)
                    if (epoch + 1) % CHECKPOINT_SAVE_FREQUENCY == 0 or epoch == training_epochs - 1 or \
                       (no_improve_counter > 0 and no_improve_counter % early_stopping_patience == 0): # Save if early stopping also triggered
                        checkpoint_path = os.path.join(output_folder, f"checkpoint_fold{fold_idx + 1}_epoch{epoch + 1}.pth")
                        torch.save({
                            'epoch': epoch + 1,
                            'model_state_dict': model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'train_loss_history': train_losses_history_fold,
                            'val_loss_history': val_losses_history_fold,
                            'train_acc_history': train_accuracies_history_fold,
                            'val_acc_history': val_accuracies_history_fold,
                        }, checkpoint_path)
                        print(f"    Checkpoint saved to {checkpoint_path}")

                # Ensure the last epoch's validation accuracy is appended if training loop completed normally
                # or if early stopping broke it (it's appended within the early stopping check)
                if not (no_improve_counter >= early_stopping_patience): # If early stopping did NOT trigger a break
                    if val_accuracies_history_fold: # Ensure there's a value
                        cv_accuracies.append(val_accuracies_history_fold[-1]) # Append the last one

            # --- End of Fold ---
            print(f"Fold {fold_idx + 1} Training Finished!")

            # Calculate and accumulate confusion matrix for this fold
            # Only if validation data was processed (i.e., training loop was not skipped by full completion)
            if len(all_val_true_fold) > 0: # all_val_true_fold is populated inside the validation loop
                conf_matrix_fold = sk_metrics.confusion_matrix(all_val_true_fold, all_val_preds_fold, labels=range(n_classes))
                confusion_matrix_sum += conf_matrix_fold
                print(f"Confusion Matrix for Fold {fold_idx + 1}:\n{conf_matrix_fold}")
            else:
                print(f"    No validation data processed for confusion matrix in Fold {fold_idx + 1} (training loop might have been skipped or no data).")

            # --- Save Performance curves for this fold ---
            # Plotting uses the historical lists. Check if any history exists to plot
            if train_losses_history_fold and val_losses_history_fold: # Both should be non-empty if training occurred
                # Loss Curve
                plt.figure(figsize=(10, 5))
                plt.plot(range(1, len(train_losses_history_fold) + 1), train_losses_history_fold, label="Train Loss")
                plt.plot(range(1, len(val_losses_history_fold) + 1), val_losses_history_fold, label="Validation Loss")
                plt.xlabel("Epoch")
                plt.ylabel("Loss")
                plt.legend()
                plt.ylim([0, 2]) # Set fixed y-axis for better comparison
                plt.title(f"Loss Curve - Fold {fold_idx + 1}")
                plt.savefig(os.path.join(output_folder, f"Loss_Fold{fold_idx + 1}.png"), dpi=150)
                plt.close()

                # Accuracy Curve
                plt.figure(figsize=(10, 5))
                plt.plot(range(1, len(train_accuracies_history_fold) + 1), train_accuracies_history_fold, label="Train Accuracy")
                plt.plot(range(1, len(val_accuracies_history_fold) + 1), val_accuracies_history_fold, label="Validation Accuracy")
                plt.xlabel("Epoch")
                plt.ylabel("Accuracy")
                plt.legend()
                plt.ylim([0, 1]) # Set fixed y-axis for better comparison
                plt.title(f"Accuracy Curve - Fold {fold_idx + 1}")
                plt.savefig(os.path.join(output_folder, f"Accuracy_Fold{fold_idx + 1}.png"), dpi=150)
                plt.close()
            else:
                print(f"    No training/validation history to plot for Fold {fold_idx + 1} (training loop might have been skipped or no data).")

    except KeyboardInterrupt:
        print("\nTraining interrupted by user (Ctrl+C). Exiting gracefully.")
        sys.exit(0) # Exit the script cleanly
    except Exception as e:
        print(f"\nCRITICAL ERROR during K-Fold Cross-Validation: {e}")
        import traceback
        traceback.print_exc() # Print full traceback for debugging
        sys.exit(1) # Exit with an error code indicating failure

    print("\n--- Cross-Validation Finished! ---")
    print(f"Average Cross-Validation Accuracy: {np.mean(cv_accuracies):.4f} +/- {np.std(cv_accuracies):.4f}")
    print("Total Confusion Matrix (summed across all folds):\n", confusion_matrix_sum)

    # Save overall confusion matrix
    np.save(os.path.join(output_folder, "total_confusion_matrix.npy"), confusion_matrix_sum)